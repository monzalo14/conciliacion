---
title: "Calibración de Random Forest para Modelos de Probabilidad"
author: "Mónica Zamudio"
date: "29 de marzo de 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
library(e1071)
library(unbalanced)
library(randomForest)
library(stringr)
library(stringdist)
library(kimisc)
library(readxl)
```

```{r, echo = F}
df_laudo <- readRDS('../clean_data/observaciones_selected_laudos.RDS') %>% 
  select(-modo_termino, -starts_with('junta'), -codem, -c_sal_caidos) 

df_laudo %>%
select(starts_with('giro')) %>%
sapply(sum) -> giros

quita <- giros[giros < 10] %>% names()
df_laudo <- df_laudo %>%
            mutate(giro_empresa_00 = rowSums(df_laudo[names(df_laudo) %in% quita])) %>%
            select(-one_of(quita[-1]))
```

Usamos la metodología *SMOTE* para tener un dataset balanceado (vamos a sobrerrepresentar algunos casos ganadores y subrepresentar algunos casos perdedores):

```{r, echo = F}
set.seed(140693)
df_laudo %>%
  select(-laudo_gana) %>%
  ubSMOTE(., df_laudo$laudo_gana, perc.over = 200, k = 5, 
  perc.under = 200, verbose = TRUE) -> listas

X <- listas$X %>% na.roughfix()
Y <- listas$Y %>% na.roughfix()

set.seed(140693)
df_laudo %>%
  select(-laudo_gana, -renuncia_voluntaria) %>%
  ubSMOTE(., df_laudo$laudo_gana, perc.over = 200, k = 5, 
  perc.under = 200, verbose = TRUE) -> listas_rm

X_rm <- listas_rm$X %>% na.roughfix()
Y_rm <- listas_rm$Y %>% na.roughfix()
```

Dividimos la muestra expandida en dos poblaciones: entrenamiento y prueba.

```{r}
set.seed(140693)
smp_size <- floor(0.80 * nrow(X))
train_ind <- sample(seq_len(nrow(X)), size = smp_size, replace = FALSE)
X_train <- X[train_ind, ]
X_test  <- X[-train_ind, ]
Y_train <- Y[train_ind]
Y_test  <- Y[-train_ind]

X_rm_train <- X_rm[train_ind, ]
X_rm_test  <- X_rm[-train_ind, ]
```

Calibramos dos Random Forest, uno con la variable de renuncia voluntaria y otro sin la variable.

```{r}
RF <- tune.randomForest(X_train, Y_train, ntree = c(900, 1000, 1100, 1200, 1300, 1400, 1500))
summary(RF)
```

Ahora, usamos el conjunto de prueba para comprobar la capacidad de generalización de nuestro modelo:
```{r}
set.seed(140693)
RF_best <- randomForest(X_train, Y_train, ntree = RF$best.parameters[[1]])
prediccion_RF <- predict(RF_best, X_test)
```

Para este modelo, tenemos entonces una tasa de error de `r length(which(prediccion_RF != Y_test))/length(Y_test)`%.

Va la lista de variables que usa el RF:
```{r}
names(X)
save(RF_best, file = 'probabilidad_laudo.RData')
```
